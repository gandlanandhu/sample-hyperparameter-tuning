{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575a07f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.0 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.0 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.2/11.0 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.4/11.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.3-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.4/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 4.3 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86c5fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.7 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/8.7 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.8/8.7 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.7 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.7/8.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 6.4 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.2-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/38.6 MB 9.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.2/38.6 MB 9.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.3/38.6 MB 9.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 8.7/38.6 MB 9.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 11.0/38.6 MB 10.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 12.8/38.6 MB 9.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 14.7/38.6 MB 9.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 9.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.3/38.6 MB 9.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.5/38.6 MB 8.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.5/38.6 MB 8.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.8/38.6 MB 6.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 6.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 17.3/38.6 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 17.6/38.6 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 18.1/38.6 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.4/38.6 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.9/38.6 MB 4.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 19.4/38.6 MB 4.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 19.9/38.6 MB 4.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 20.7/38.6 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 21.2/38.6 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.0/38.6 MB 4.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 22.8/38.6 MB 4.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.6/38.6 MB 4.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.4/38.6 MB 4.3 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 25.4/38.6 MB 4.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 26.5/38.6 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.8/38.6 MB 4.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.8/38.6 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 30.1/38.6 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.2/38.6 MB 4.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.5/38.6 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 33.8/38.6 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.4/38.6 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.7/38.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 4.5 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "612b9cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-win_amd64.whl (241.3 MB)\n",
      "   ---------------------------------------- 0.0/241.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/241.3 MB 8.4 MB/s eta 0:00:29\n",
      "    --------------------------------------- 3.9/241.3 MB 10.2 MB/s eta 0:00:24\n",
      "    --------------------------------------- 6.0/241.3 MB 9.5 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 8.4/241.3 MB 9.8 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 10.5/241.3 MB 9.8 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 12.3/241.3 MB 9.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 14.4/241.3 MB 9.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 16.3/241.3 MB 9.6 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 17.6/241.3 MB 9.2 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 18.4/241.3 MB 8.9 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 18.6/241.3 MB 8.2 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 18.9/241.3 MB 7.6 MB/s eta 0:00:30\n",
      "   --- ------------------------------------ 19.4/241.3 MB 6.8 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 19.7/241.3 MB 6.6 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 20.2/241.3 MB 6.2 MB/s eta 0:00:36\n",
      "   --- ------------------------------------ 20.7/241.3 MB 6.0 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 21.2/241.3 MB 5.8 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 21.8/241.3 MB 5.6 MB/s eta 0:00:40\n",
      "   --- ------------------------------------ 22.5/241.3 MB 5.5 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 23.1/241.3 MB 5.4 MB/s eta 0:00:41\n",
      "   --- ------------------------------------ 23.9/241.3 MB 5.3 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 24.6/241.3 MB 5.2 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 25.7/241.3 MB 5.2 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 26.5/241.3 MB 5.1 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 27.5/241.3 MB 5.1 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 28.6/241.3 MB 5.1 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 29.6/241.3 MB 5.1 MB/s eta 0:00:42\n",
      "   ----- ---------------------------------- 30.9/241.3 MB 5.1 MB/s eta 0:00:42\n",
      "   ----- ---------------------------------- 32.0/241.3 MB 5.1 MB/s eta 0:00:42\n",
      "   ----- ---------------------------------- 33.0/241.3 MB 5.1 MB/s eta 0:00:41\n",
      "   ----- ---------------------------------- 34.3/241.3 MB 5.1 MB/s eta 0:00:41\n",
      "   ----- ---------------------------------- 35.4/241.3 MB 5.1 MB/s eta 0:00:41\n",
      "   ------ --------------------------------- 37.0/241.3 MB 5.2 MB/s eta 0:00:40\n",
      "   ------ --------------------------------- 38.3/241.3 MB 5.2 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 39.8/241.3 MB 5.3 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 41.2/241.3 MB 5.3 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 42.5/241.3 MB 5.3 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 44.0/241.3 MB 5.4 MB/s eta 0:00:37\n",
      "   ------- -------------------------------- 45.9/241.3 MB 5.4 MB/s eta 0:00:36\n",
      "   ------- -------------------------------- 47.4/241.3 MB 5.5 MB/s eta 0:00:36\n",
      "   -------- ------------------------------- 48.8/241.3 MB 5.5 MB/s eta 0:00:35\n",
      "   -------- ------------------------------- 50.3/241.3 MB 5.6 MB/s eta 0:00:35\n",
      "   -------- ------------------------------- 51.9/241.3 MB 5.6 MB/s eta 0:00:34\n",
      "   -------- ------------------------------- 53.5/241.3 MB 5.6 MB/s eta 0:00:34\n",
      "   --------- ------------------------------ 55.3/241.3 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 56.9/241.3 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 57.9/241.3 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 58.5/241.3 MB 5.7 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 59.2/241.3 MB 5.6 MB/s eta 0:00:33\n",
      "   --------- ------------------------------ 60.0/241.3 MB 5.6 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 61.1/241.3 MB 5.5 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 62.1/241.3 MB 5.5 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 62.9/241.3 MB 5.5 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 64.2/241.3 MB 5.5 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 65.3/241.3 MB 5.5 MB/s eta 0:00:33\n",
      "   ---------- ----------------------------- 66.3/241.3 MB 5.5 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 67.6/241.3 MB 5.5 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 68.9/241.3 MB 5.5 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 70.3/241.3 MB 5.5 MB/s eta 0:00:32\n",
      "   ----------- ---------------------------- 71.6/241.3 MB 5.5 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 73.1/241.3 MB 5.5 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 73.7/241.3 MB 5.5 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 74.2/241.3 MB 5.5 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 74.7/241.3 MB 5.4 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 75.2/241.3 MB 5.4 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 76.0/241.3 MB 5.3 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 76.8/241.3 MB 5.3 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 77.6/241.3 MB 5.3 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 78.4/241.3 MB 5.3 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 79.4/241.3 MB 5.3 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 80.2/241.3 MB 5.2 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 81.3/241.3 MB 5.2 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 82.3/241.3 MB 5.2 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 83.4/241.3 MB 5.2 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 84.4/241.3 MB 5.2 MB/s eta 0:00:31\n",
      "   -------------- ------------------------- 85.7/241.3 MB 5.2 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 87.0/241.3 MB 5.2 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 88.1/241.3 MB 5.2 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 89.7/241.3 MB 5.3 MB/s eta 0:00:29\n",
      "   --------------- ------------------------ 91.2/241.3 MB 5.3 MB/s eta 0:00:29\n",
      "   --------------- ------------------------ 92.8/241.3 MB 5.3 MB/s eta 0:00:29\n",
      "   --------------- ------------------------ 94.4/241.3 MB 5.3 MB/s eta 0:00:28\n",
      "   --------------- ------------------------ 95.4/241.3 MB 5.3 MB/s eta 0:00:28\n",
      "   ---------------- ----------------------- 97.0/241.3 MB 5.3 MB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 98.6/241.3 MB 5.4 MB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 99.9/241.3 MB 5.4 MB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 100.7/241.3 MB 5.4 MB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 101.4/241.3 MB 5.3 MB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 102.0/241.3 MB 5.3 MB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 102.8/241.3 MB 5.3 MB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 103.5/241.3 MB 5.3 MB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 104.3/241.3 MB 5.3 MB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 105.4/241.3 MB 5.2 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 106.2/241.3 MB 5.2 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 107.2/241.3 MB 5.2 MB/s eta 0:00:26\n",
      "   ----------------- ---------------------- 108.3/241.3 MB 5.2 MB/s eta 0:00:26\n",
      "   ------------------ --------------------- 109.6/241.3 MB 5.2 MB/s eta 0:00:26\n",
      "   ------------------ --------------------- 110.6/241.3 MB 5.2 MB/s eta 0:00:25\n",
      "   ------------------ --------------------- 111.9/241.3 MB 5.2 MB/s eta 0:00:25\n",
      "   ------------------ --------------------- 113.2/241.3 MB 5.2 MB/s eta 0:00:25\n",
      "   ------------------ --------------------- 114.6/241.3 MB 5.3 MB/s eta 0:00:25\n",
      "   ------------------- -------------------- 115.3/241.3 MB 5.2 MB/s eta 0:00:24\n",
      "   ------------------- -------------------- 116.4/241.3 MB 5.2 MB/s eta 0:00:24\n",
      "   ------------------- -------------------- 117.4/241.3 MB 5.2 MB/s eta 0:00:24\n",
      "   ------------------- -------------------- 118.8/241.3 MB 5.2 MB/s eta 0:00:24\n",
      "   ------------------- -------------------- 119.5/241.3 MB 5.2 MB/s eta 0:00:24\n",
      "   -------------------- ------------------- 120.8/241.3 MB 5.2 MB/s eta 0:00:24\n",
      "   -------------------- ------------------- 122.2/241.3 MB 5.2 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 123.5/241.3 MB 5.3 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 124.8/241.3 MB 5.3 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 126.4/241.3 MB 5.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 127.7/241.3 MB 5.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 128.5/241.3 MB 5.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 129.0/241.3 MB 5.3 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 129.8/241.3 MB 5.2 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 130.5/241.3 MB 5.2 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 131.3/241.3 MB 5.2 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 132.1/241.3 MB 5.2 MB/s eta 0:00:22\n",
      "   ---------------------- ----------------- 133.2/241.3 MB 5.2 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 134.2/241.3 MB 5.2 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 135.3/241.3 MB 5.2 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 136.6/241.3 MB 5.2 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 137.9/241.3 MB 5.2 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 138.9/241.3 MB 5.2 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 140.2/241.3 MB 5.2 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 141.6/241.3 MB 5.2 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 142.9/241.3 MB 5.2 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 144.2/241.3 MB 5.2 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 145.8/241.3 MB 5.2 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 147.1/241.3 MB 5.2 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 147.6/241.3 MB 5.2 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 148.1/241.3 MB 5.2 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 148.9/241.3 MB 5.2 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 149.4/241.3 MB 5.2 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 150.2/241.3 MB 5.2 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 151.0/241.3 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 151.5/241.3 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 152.0/241.3 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 152.8/241.3 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 153.6/241.3 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 154.4/241.3 MB 5.0 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 155.2/241.3 MB 5.0 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 156.2/241.3 MB 4.9 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 157.3/241.3 MB 4.9 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 158.1/241.3 MB 4.9 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 159.4/241.3 MB 4.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 160.4/241.3 MB 4.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 161.7/241.3 MB 4.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 162.8/241.3 MB 4.8 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 164.4/241.3 MB 4.9 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 165.7/241.3 MB 4.9 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 167.0/241.3 MB 4.9 MB/s eta 0:00:16\n",
      "   --------------------------- ------------ 168.6/241.3 MB 5.0 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 169.6/241.3 MB 5.0 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 171.2/241.3 MB 5.0 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 172.8/241.3 MB 5.1 MB/s eta 0:00:14\n",
      "   ---------------------------- ----------- 174.1/241.3 MB 5.1 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 175.9/241.3 MB 5.1 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 177.5/241.3 MB 5.2 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 179.3/241.3 MB 5.2 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 180.1/241.3 MB 5.2 MB/s eta 0:00:12\n",
      "   ----------------------------- ---------- 180.9/241.3 MB 5.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 181.4/241.3 MB 5.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 182.2/241.3 MB 5.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 183.0/241.3 MB 5.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 183.8/241.3 MB 5.1 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 184.3/241.3 MB 5.1 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 185.1/241.3 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 185.9/241.3 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 186.6/241.3 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 187.4/241.3 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 188.2/241.3 MB 5.1 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 189.0/241.3 MB 5.0 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 189.5/241.3 MB 5.0 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 190.3/241.3 MB 5.0 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 191.1/241.3 MB 5.0 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 192.2/241.3 MB 4.9 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 192.9/241.3 MB 4.9 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 194.0/241.3 MB 4.9 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 195.0/241.3 MB 4.9 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 195.8/241.3 MB 4.9 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 197.1/241.3 MB 4.9 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 198.4/241.3 MB 4.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 199.5/241.3 MB 4.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 200.8/241.3 MB 4.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 202.4/241.3 MB 4.8 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 203.7/241.3 MB 4.8 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 205.0/241.3 MB 4.9 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 206.6/241.3 MB 4.9 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 207.9/241.3 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 209.5/241.3 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 211.0/241.3 MB 4.9 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 212.3/241.3 MB 5.0 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 214.2/241.3 MB 5.0 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 215.7/241.3 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 217.3/241.3 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 218.6/241.3 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 219.2/241.3 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 219.4/241.3 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 219.9/241.3 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 220.7/241.3 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 221.2/241.3 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 222.0/241.3 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 222.8/241.3 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 223.6/241.3 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 224.7/241.3 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 225.4/241.3 MB 4.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 226.5/241.3 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 227.8/241.3 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 229.1/241.3 MB 5.0 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 230.2/241.3 MB 5.0 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 231.5/241.3 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 232.8/241.3 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 233.8/241.3 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  235.4/241.3 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  237.0/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.3/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  239.9/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.3/241.3 MB 4.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/6.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.7/6.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 5.3 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 5.7 MB/s eta 0:00:00\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.8.0 typing-extensions-4.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41fefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c376bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/DELL/Desktop/sample-hyperparameter-tuning/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "576f1638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87534969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2701e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,:-1].values\n",
    "y=data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138ba459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bef127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e87c9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class DiabetesClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DiabetesClassifier, self).__init__()\n",
    "    self.linear_model=nn.Sequential(\n",
    "        nn.Linear(in_features=8,out_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=128,out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64,out_features=1)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    return self.linear_model(x)\n",
    "model=DiabetesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b1c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(params=model.parameters(),lr=0.01)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cbe9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_pred, y_true):\n",
    "    preds = torch.round(torch.sigmoid(y_pred))\n",
    "    correct = (preds == y_true).float()\n",
    "    return correct.sum() / len(correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4de57c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_acc:34.69, test_acc:35.71\n",
      "epoch:10, train_acc:34.69, test_acc:35.71\n",
      "epoch:20, train_acc:34.69, test_acc:35.71\n",
      "epoch:30, train_acc:34.69, test_acc:35.71\n",
      "epoch:40, train_acc:34.69, test_acc:35.71\n",
      "epoch:50, train_acc:34.69, test_acc:35.71\n",
      "epoch:60, train_acc:35.02, test_acc:35.71\n",
      "epoch:70, train_acc:35.34, test_acc:35.71\n",
      "epoch:80, train_acc:35.83, test_acc:37.66\n",
      "epoch:90, train_acc:37.79, test_acc:40.26\n",
      "epoch:100, train_acc:40.88, test_acc:42.86\n",
      "epoch:110, train_acc:43.32, test_acc:42.86\n",
      "epoch:120, train_acc:46.25, test_acc:45.45\n",
      "epoch:130, train_acc:48.21, test_acc:45.45\n",
      "epoch:140, train_acc:50.00, test_acc:45.45\n",
      "epoch:150, train_acc:50.81, test_acc:46.10\n",
      "epoch:160, train_acc:51.30, test_acc:46.75\n",
      "epoch:170, train_acc:52.12, test_acc:47.40\n",
      "epoch:180, train_acc:53.09, test_acc:47.40\n",
      "epoch:190, train_acc:53.75, test_acc:47.40\n"
     ]
    }
   ],
   "source": [
    "epochs=200\n",
    "random_state=42\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  y_logits=model(torch.tensor(X_train).float())\n",
    "  y_pred=torch.sigmoid(y_logits)\n",
    "  train_loss = loss_fn(y_logits, torch.tensor(y_train).float().unsqueeze(1))\n",
    "  train_acc=accuracy_fn(y_pred,torch.tensor(y_train).float().unsqueeze(1))\n",
    "  optimizer.zero_grad()\n",
    "  train_loss.backward()\n",
    "  optimizer.step()\n",
    "  model.eval()\n",
    "  with torch.inference_mode():\n",
    "    test_logits=model(torch.tensor(X_test).float())\n",
    "    test_pred=torch.sigmoid(test_logits)\n",
    "    test_loss = loss_fn(test_logits, torch.tensor(y_test).float().unsqueeze(1))\n",
    "    test_acc=accuracy_fn(test_pred,torch.tensor(y_test).float().unsqueeze(1))\n",
    "    if epoch%10==0:\n",
    "      print(f\"epoch:{epoch}, train_acc:{train_acc.item()*100:.2f}, test_acc:{test_acc.item()*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e56631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning with different optimizers\n",
    "def train_with_optimizer(optimizer_name, X_train, y_train, X_val, y_val, epochs=50, lr=0.001):\n",
    "    model = DiabetesClassifier()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # choose optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"adadelta\":\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_logits = model(torch.tensor(X_train).float())\n",
    "        loss = loss_fn(y_logits, torch.tensor(y_train).float().unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(torch.tensor(X_val).float())\n",
    "        val_acc = accuracy_fn(torch.sigmoid(val_logits), torch.tensor(y_val).float().unsqueeze(1)).item()\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69530fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer=adam, Val Accuracy=0.3571\n",
      "Optimizer=sgd, Val Accuracy=0.3571\n",
      "Optimizer=rmsprop, Val Accuracy=0.3571\n",
      "Optimizer=adadelta, Val Accuracy=0.3571\n",
      "\n",
      "Best Optimizer: adam with Val Accuracy: 0.3571428656578064\n"
     ]
    }
   ],
   "source": [
    "optimizers = [\"adam\", \"sgd\", \"rmsprop\", \"adadelta\"]\n",
    "\n",
    "best_acc = 0\n",
    "best_opt = None\n",
    "\n",
    "for opt in optimizers:\n",
    "    acc = train_with_optimizer(opt, X_train, y_train, X_test, y_test, epochs=50, lr=0.01)\n",
    "    print(f\"Optimizer={opt}, Val Accuracy={acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_opt = opt\n",
    "\n",
    "print(\"\\nBest Optimizer:\", best_opt, \"with Val Accuracy:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d03a3bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning no. of hidden layers,hidden units and learning rate\n",
    "class DiabetesClassifier1(nn.Module):\n",
    "    def __init__(self, input_dim=8, hidden_layers=2, hidden_units=32):\n",
    "        super(DiabetesClassifier1, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        # Add hidden layers dynamically\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = hidden_units\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95de2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "def train_model(hidden_layers, hidden_units, lr, epochs=50):\n",
    "    model = DiabetesClassifier1(hidden_layers=hidden_layers, hidden_units=hidden_units)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_logits = model(torch.tensor(X_train).float())\n",
    "        loss = loss_fn(y_logits, torch.tensor(y_train).float().unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(torch.tensor(X_test).float())\n",
    "        val_preds = torch.sigmoid(val_logits)\n",
    "        val_acc = accuracy_fn(val_preds, torch.tensor(y_test).float().unsqueeze(1)) # Pass val_preds as a tensor\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "590475c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers=1, Units=16, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=256, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=256, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, LR=0.01, Val Acc=0.3701\n",
      "Layers=2, Units=128, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=256, LR=0.01, Val Acc=0.3701\n",
      "Layers=2, Units=256, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, LR=0.01, Val Acc=0.3831\n",
      "Layers=3, Units=64, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, LR=0.01, Val Acc=0.3831\n",
      "Layers=3, Units=128, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=256, LR=0.01, Val Acc=0.3701\n",
      "Layers=3, Units=256, LR=0.001, Val Acc=0.3571\n",
      "Layers=4, Units=16, LR=0.01, Val Acc=0.3571\n",
      "Layers=4, Units=16, LR=0.001, Val Acc=0.3571\n",
      "Layers=4, Units=32, LR=0.01, Val Acc=0.3701\n",
      "Layers=4, Units=32, LR=0.001, Val Acc=0.3571\n",
      "Layers=4, Units=64, LR=0.01, Val Acc=0.3701\n",
      "Layers=4, Units=64, LR=0.001, Val Acc=0.3571\n",
      "Layers=4, Units=128, LR=0.01, Val Acc=0.4026\n",
      "Layers=4, Units=128, LR=0.001, Val Acc=0.3571\n",
      "Layers=4, Units=256, LR=0.01, Val Acc=0.3701\n",
      "Layers=4, Units=256, LR=0.001, Val Acc=0.3701\n",
      "Layers=5, Units=16, LR=0.01, Val Acc=0.3571\n",
      "Layers=5, Units=16, LR=0.001, Val Acc=0.3571\n",
      "Layers=5, Units=32, LR=0.01, Val Acc=0.3571\n",
      "Layers=5, Units=32, LR=0.001, Val Acc=0.3571\n",
      "Layers=5, Units=64, LR=0.01, Val Acc=0.3701\n",
      "Layers=5, Units=64, LR=0.001, Val Acc=0.3571\n",
      "Layers=5, Units=128, LR=0.01, Val Acc=0.3961\n",
      "Layers=5, Units=128, LR=0.001, Val Acc=0.3571\n",
      "Layers=5, Units=256, LR=0.01, Val Acc=0.3571\n",
      "Layers=5, Units=256, LR=0.001, Val Acc=0.3571\n",
      "\n",
      "Best Hyperparameters (Adam): {'layers': 4, 'units': 128, 'lr': 0.01} with Val Accuracy: tensor(0.4026)\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_options = [1, 2, 3,4,5]\n",
    "hidden_units_options = [16, 32, 64, 128,256]\n",
    "learning_rates = [0.01, 0.001]\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "for hl in hidden_layers_options:\n",
    "    for hu in hidden_units_options:\n",
    "        for lr in learning_rates:\n",
    "            acc = train_model(hl, hu, lr, epochs=50)\n",
    "            print(f\"Layers={hl}, Units={hu}, LR={lr}, Val Acc={acc:.4f}\")\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = {\"layers\": hl, \"units\": hu, \"lr\": lr}\n",
    "\n",
    "print(\"\\nBest Hyperparameters (Adam):\", best_params, \"with Val Accuracy:\", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ec7cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#different activation functions\n",
    "class DiabetesClassifier2(nn.Module):\n",
    "    def __init__(self, activation_fn=nn.ReLU):\n",
    "        super(DiabetesClassifier2, self).__init__() # Corrected the class name in super()\n",
    "        self.linear_model = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=128),\n",
    "            activation_fn(),  # activation function is dynamic here\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            activation_fn(),\n",
    "            nn.Linear(in_features=64, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ec196f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(activation_fn, lr=0.001, epochs=50):\n",
    "    model = DiabetesClassifier2(activation_fn=activation_fn)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_logits = model(torch.tensor(X_train).float())\n",
    "        loss = loss_fn(y_logits, torch.tensor(y_train).float().unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(torch.tensor(X_test).float())\n",
    "        val_preds = torch.round(torch.sigmoid(val_logits))\n",
    "        val_acc = accuracy_fn(val_preds, torch.tensor(y_test).float().unsqueeze(1)) # Pass val_preds as a tensor and y_test as a tensor\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23eff9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation=ReLU, Validation Accuracy=0.7662\n",
      "Activation=Sigmoid, Validation Accuracy=0.6429\n",
      "Activation=Tanh, Validation Accuracy=0.7597\n",
      "Activation=LeakyReLU, Validation Accuracy=0.7532\n",
      "Activation=ELU, Validation Accuracy=0.7532\n",
      "\n",
      "Best Activation Function: ReLU with Validation Accuracy: 0.7662\n"
     ]
    }
   ],
   "source": [
    "activations = {\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "    \"Tanh\": nn.Tanh,\n",
    "    \"LeakyReLU\": nn.LeakyReLU,\n",
    "    \"ELU\": nn.ELU\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_activation = \"\"\n",
    "\n",
    "for name, act in activations.items():\n",
    "    acc = train_model(act, lr=0.001, epochs=50)\n",
    "    print(f\"Activation={name}, Validation Accuracy={acc:.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_activation = name\n",
    "\n",
    "print(f\"\\nBest Activation Function: {best_activation} with Validation Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2894d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing optimizers,no. of hidden layers,hidden units,activation functions\n",
    "class DiabetesClassifier3(nn.Module):\n",
    "    def __init__(self, input_dim=8, hidden_layers=2, hidden_units=32, activation_fn=nn.ReLU):\n",
    "        super(DiabetesClassifier3, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        # dynamically add hidden layers\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(activation_fn())  # variable activation\n",
    "            in_features = hidden_units\n",
    "\n",
    "        # output layer (no activation here)\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ecf87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(hidden_layers, hidden_units, activation_fn, optimizer_name, lr=0.001, epochs=50):\n",
    "    model = DiabetesClassifier3(hidden_layers=hidden_layers, hidden_units=hidden_units, activation_fn=activation_fn)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # choose optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"adadelta\":\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        y_logits = model(torch.tensor(X_train).float())\n",
    "        loss = loss_fn(y_logits, torch.tensor(y_train).float().unsqueeze(1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(torch.tensor(X_test).float())\n",
    "        val_preds = torch.sigmoid(val_logits)\n",
    "        val_acc = accuracy_fn(val_preds, torch.tensor(y_test).float().unsqueeze(1)) # Pass val_preds and y_test as tensors\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e687b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers=1, Units=16, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=16, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=32, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=64, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=1, Units=128, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=16, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=32, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=64, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3701\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3766\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=2, Units=128, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=16, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3701\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=32, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3701\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3766\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=64, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=adam, LR=0.01, Val Acc=0.3701\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Sigmoid, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=Tanh, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=adam, LR=0.01, Val Acc=0.3636\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=LeakyReLU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=adam, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=adam, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=sgd, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=sgd, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=rmsprop, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=rmsprop, LR=0.001, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=adadelta, LR=0.01, Val Acc=0.3571\n",
      "Layers=3, Units=128, Activation=ELU, Optimizer=adadelta, LR=0.001, Val Acc=0.3571\n",
      "\n",
      "Best Hyperparameters:\n",
      "hidden_layers: 2\n",
      "hidden_units: 128\n",
      "activation: LeakyReLU\n",
      "optimizer: adam\n",
      "lr: 0.01\n",
      "Best Validation Accuracy: 0.3766\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_options = [1, 2, 3]\n",
    "hidden_units_options = [16, 32, 64, 128]\n",
    "activation_functions = {\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"Sigmoid\": nn.Sigmoid,\n",
    "    \"Tanh\": nn.Tanh,\n",
    "    \"LeakyReLU\": nn.LeakyReLU,\n",
    "    \"ELU\": nn.ELU\n",
    "}\n",
    "optimizers = [\"adam\", \"sgd\", \"rmsprop\", \"adadelta\"]\n",
    "learning_rates = [0.01, 0.001]\n",
    "\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "for hl in hidden_layers_options:\n",
    "    for hu in hidden_units_options:\n",
    "        for act_name, act_fn in activation_functions.items():\n",
    "            for opt in optimizers:\n",
    "                for lr in learning_rates:\n",
    "                    acc = train_model(hl, hu, act_fn, opt, lr=lr, epochs=50)\n",
    "                    print(f\"Layers={hl}, Units={hu}, Activation={act_name}, Optimizer={opt}, LR={lr}, Val Acc={acc:.4f}\")\n",
    "                    if acc > best_acc:\n",
    "                        best_acc = acc\n",
    "                        best_params = {\n",
    "                            \"hidden_layers\": hl,\n",
    "                            \"hidden_units\": hu,\n",
    "                            \"activation\": act_name,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"lr\": lr\n",
    "                        }\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(f\"Best Validation Accuracy: {best_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
